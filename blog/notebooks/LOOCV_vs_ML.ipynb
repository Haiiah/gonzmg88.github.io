{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leave one Out Cross Validation vs Marginal Likelihood\n",
    "\n",
    "Let \\\\( \\mathcal{D}=(X,y)\\\\) be the data of a regression/classification problem. \n",
    "\n",
    "The __marginal likelihood (ML)__ is the probability of the data for a given _model_: $$ ML(\\mathcal{D}) = p(y \\mid X) $$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The __leave-one-out marginal likelihood (LOOML)__  is defined as:\n",
    "$$\n",
    "\\text{LOOML}(\\mathcal{D}) = \\prod_{i=1}^N p(y_i \\mid \\overbrace{X_{(i)},x_i}^{X},y_{(i)})\n",
    "$$\n",
    "Where $y_{(i)}$ is the vector without the $i-th$ variable:  $y_{(i)} = (y_1,..,y_{i-1},y_{i+1},...,y_N)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How the ML is related to LOOML? We can decompose the ML to resemble to the LOOML:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "ML(\\mathcal{D}) =p(y \\mid X) &= p(y_1 \\mid y_2,..,y_N,X)p(y_2,..,y_N \\mid X)\\\\ \n",
    "&= p(y_1 \\mid y_2,..,y_N,X)p(y_2 \\mid y_3,..,y_N, X) p(y_3,...,y_N \\mid X) = \\\\\n",
    "&= p(y_1 \\mid y_2,..,y_N,X)p(y_2 \\mid y_3,..,y_N, X) p(y_3 \\mid y_4,...,y_N, X)p(y_4,...,y_N \\mid X) = ...\\\\\n",
    "&\\vdots \\\\\n",
    "&= \\prod_{i=1}^N p(y_i \\mid y_{i+1},...,y_N,X)\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let call the vectors: $\\text{prev}_i = (y_1,...,y_{i-1})$ $\\text{post}_i = (y_{i+1},...,y_{N})$ the ML and the LOOML can be written as:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "LOOML(\\mathcal{D}) &= \\prod_{i=1}^N p(y_i \\mid \\text{prev}_i, \\text{post}_i, X) \\\\\n",
    "ML(\\mathcal{D}) &= \\prod_{i=1}^N p(y_i \\mid \\text{post}_i, X)\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore to find a relation we just have to find the relation between the probabilities: $p(y_i \\mid \\text{prev}_i,\\text{post}_i,X)$ and $p(y_i \\mid \\text{post}_i,X)$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "p(y_i \\mid \\text{prev}_i,\\text{post}_i,X) &=p(y_i \\mid \\text{prev}_i,\\text{post}_i,X) \\frac{p(\\text{prev}_i \\mid \\text{post}_i,X)}{p(\\text{prev}_i \\mid \\text{post}_i,X)} \\\\\n",
    "&= \\frac{p(y_i, \\text{prev}_i  \\mid \\text{post}_i,X)}{p(\\text{prev}_i \\mid \\text{post}_i,X)} \\\\\n",
    "&= \\frac{p(\\text{prev}_i  \\mid y_i, \\text{post}_i, X) p(y_i \\mid  \\text{post}_i, X)}{p(\\text{prev}_i \\mid \\text{post}_i,X)}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we plug this in the LOOML expression we have:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "LOOML(\\mathcal{D}) &= \\prod_{i=1}^N p(y_i \\mid \\text{prev}_i, \\text{post}_i, X) \\\\\n",
    "&= \\prod_{i=1}^N p(y_i \\mid  \\text{post}_i, X) \\frac{p(\\text{prev}_i  \\mid y_i, \\text{post}_i, X)}{p(\\text{prev}_i \\mid \\text{post}_i,X)} \\\\\n",
    "&= ML(\\mathcal{D}) \\prod_{i=1}^N \\frac{p(\\text{prev}_i  \\mid y_i, \\text{post}_i, X)}{p(\\text{prev}_i \\mid \\text{post}_i,X)}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LOOML seems like an *strange construction* of the *more frequentist approach* __Leave one out error__:\n",
    "\n",
    "$$\n",
    "LOO_{err}(\\mathcal{D})= \\sum_{i=1}^N loss(y_i,pred(m_{(i)},x_i)) \n",
    "$$\n",
    "Where $pred(m_{(i)},x_i)$ is the prediction given by the model trained without the datapoint $i$ evaluated on the datapoint $x_i$. \n",
    "\n",
    "We can reformulate this error using the __predictive leave one out error PLOOE__ defined as:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "PLOOE(\\mathcal{D}) &= \\frac{1}{N}\\sum_{i=1}^N \\mathbb{E}_{p(t \\mid X_{(i)},x_i,y_{(i)})}\\big[loss(t,\\hat{y}_{(i)}) \\big] \\\\\n",
    "&= \\frac{1}{N}\\sum_{i=1}^N \\int loss(t,\\hat{y}_{(i)}) p(t \\mid X_{(i)},x_i, y_{(i)}) dt\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Where we call $\\hat{y}_{(i)}$ to the optimal prediction for the loss function $loss$. That is: \n",
    "\n",
    "$$\n",
    "\\hat{y}_{(i)} = \\arg \\min_{a} \\left[ \\int loss(t,a) p(t \\mid X_{(i)},x_i, y_{(i)}) dt \\right]\n",
    "$$\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
