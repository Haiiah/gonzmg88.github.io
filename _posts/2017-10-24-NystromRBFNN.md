---
layout: post
title: Why Radial Basis Function Neural Networks are equivalent to Nystrom method
date: 2017-10-24
author: Gonzalo Mateo-García
---

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
      //TeX: { equationNumbers: { autoNumber: "False" } }
    });
  /*  MathJax.Hub.Queue(
  ["resetEquationNumbers",MathJax.InputJax.TeX],
  ["PreProcess",MathJax.Hub],
  ["Reprocess",MathJax.Hub]
);*/
</script>

<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

<!-- <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-AMS-MML_HTMLorMML-full" charset="utf-8">-->

<!-- <script type="text/javascript" src="/js/MathJax.js">
</script>-->



Radial Basis Function Neural Networks (RBFN) and Nystrom methods are two methods that can be used for regression. They come from different *statistical learning* frameworks: RBFN were proposed in the context of *shallow* neural networks: this is a three layer neural network where the hidden layer computes the rbf similarities to a set of *centroids*. Then a fully connected layer is added to give the final prediction. Nystrom method was proposed in the kernel machine literature as a way to reduce the computational burden of the full kernel similarity matrix $K$. Nystrom was proposed latter, however it can be applied to different problems not only regression.

The equivalence of the method is not something unknown that we came up to; however we feel it is not crystal clear the relationship as perhaps seems for experts in the field. In [[Quiñonero-Candela 2005]](http://www.jmlr.org/papers/v6/quinonero-candela05a.html){:target="_blank"} and in Rassmussen and Williams [*Guassian Process for Machine Learning*](http://www.gaussianprocess.org/gpml/){:target="_blank"} book this relations are taken from granted. [Recent papers in the context of RBFNN](http://proceedings.mlr.press/v51/que16.html){:target="_blank"} explicitly say this. In addition, another clear proof of the people awareness of this relationship is the [scikit learn implementation of Nystrom method](http://scikit-learn.org/stable/auto_examples/plot_kernel_approximation.html#sphx-glr-auto-examples-plot-kernel-approximation-py){:target="_blank"}, which explicitly uses it. In this way the purpose of this post is to shed light in the *proof* of this relationship which helps to better understand this powerful methods. First we show the relation into an *empirical risk minimization* setting, later we will develop the *Bayesian* approach which leads to one of the called **sparse $\mathcal{GP}$** methods, we will see that the RBFN network is equivalent in this context to the **Subset of Regressors (SoR)** method (following [[Quiñonero-Candela 2005]](http://www.jmlr.org/papers/v6/quinonero-candela05a.html){:target="_blank"} naming convention). **Sparse $\mathcal{GP}$s** is an active field of research which have had many contributions over the last years.

## Introduction
Let $$ \mathcal{D}=\{x_i,y_i\}_{i=1}^N $$ be the data of our regression problem: \\(x_i\in\mathbb{R}^D\\), \\(y_i\in \mathbb{R}\\). In matrix notation we write \\(\mathcal{D}=(X,y)\\) where \\(X\\) is an $N\times D$ matrix and \\(y\\) and \\(N \times 1\\) vector. Both methods employ a subset of \\(\mathcal{D}\\) of size \\(M\\) \\(\mathcal{D}_u = (X_u,y_u)\\) (\\(X_u\\) is an \\(M\times D\\) matrix and \\(y_u\\) and $M \times 1$). This subset is sometimes called *centroids*, *pseudo-inputs* or *inducing inputs*. Also, we will sometimes refer with the subscript $f$ to the subset formed by all the data: \\(\mathcal{D}=\mathcal{D_f}=(X_f,y_f)=(X,y)\\)

As we are working with kernels we suppose we have a kernel function \\(k(x_i,x_j)\in \mathbb{R}\\) that measures similarities between points \\(x_i\in\mathbb{R}^D\\). We will define the distance between 1 point \\(x_i\\) and a bunch of points \\(X_u\\) as the row vector: \\(k(x_i,X_u) = K_{x_iu}  =[k(x_i,x_{u_1}),k(x_i,x_{u_2}),...,k(x_i,x_{u_M})] \in \mathbb{R}^{1\times M}\\). Similarly we can define the matrix generated by the distances between a bunch of points \\(X_u\\) and \\(X_f\\) as:

$$
k(X_f,X_u)=K_{fu} = \begin{pmatrix} k(x_1,x_{u_1}) & k(x_1,x_{u_2}) &...& k(x_1,x_{u_M}) \\
k(x_2,x_{u_1}) & k(x_2,x_{u_2}) &...& k(x_2,x_{u_M}) \\
\vdots & \vdots & \ddots &\vdots \\
k(x_N,x_{u_1}) & k(x_N,x_{u_2}) &...& k(x_N,x_{u_M}) \\
\end{pmatrix} \in \mathbb{R}^{N\times M}
\quad \text{(1)}
$$

## Empirical Risk Minimization

### Nystrom method

Nystrom method [[Williams and Seeger 2000]](https://papers.nips.cc/paper/1866-using-the-nystrom-method-to-speed-up-kernel-machines){:target="_blank"} proposes the substitution of the kernel function $k$ with $k_{nystrom}$ defined as $k_{nystrom}(x_i,x_j) = k(x_i,X_u)K_{uu}^{-1}k(X_u,x_j)$. The matrix $K$ is replaced then with $Q=K_{fu}K_{uu}^{-1}K_{uf}$ ($Q_{ff}$).

The Kernel Ridge Regression (KRR) solution using the kernel function $k_{nystrom}$ for a given $x^\*$ is:

$$
k_{nystrom}(x^*,X)\Big(Q_{ff}+ \sigma I \Big)^{-1} y = K_{*u} \overbrace{K_{uu}^{-1}K_{uf}\Big(Q_{ff}+ \sigma^2 I \Big)^{-1}}^{A} y \quad \text{(2)}
$$

### RBFN method

The (RBFN) method is an **empirical risk minimization approach** originally proposed by [[Poggio and Girosi 1990]](http://cbcl.mit.edu/people/poggio/journals/poggio-girosi-IEEE-1990.pdf){:target="_blank"} (eq 25). In this approach we have first to map the inputs $x$ to the space generated by the kernel similarities to $X_u$: $k(x,X_u)$. For the training input $X$ yield the matrix $K_{fu}$ of (1).

Then we solve the **linear ridge regression** problem in the transformed domain $K_{fu}$. That is, we seek to minimize  the empirical regularized risk $J$ for $\alpha$:

$$
\begin{aligned}
J(\alpha) &= \overbrace{\| K_{fu}\alpha - y\|^2}^{\text{empirical risk}} + \overbrace{\sigma^2 \alpha^t K_{uu} \alpha}^{\text{regularization}} \\ &= \alpha^t K_{uf}K_{fu} \alpha - 2 \alpha^t K_{uf} y + y^ty +  \sigma^2 \alpha^t K_{uu}\alpha
\end{aligned}
$$

Solving $\nabla_{\alpha} J = 0$ yields into:

$$
\alpha_{opt} = \Big( K_{uf}K_{fu}\ + \sigma^2 K_{uu}\Big)^{-1} K_{uf}y
$$

The prediction for a given $x^*$ is then:

$$
K_{*u}\cdot \alpha_{opt} = K_{*u} \overbrace{\Big( K_{uf}K_{fu}\ +  \sigma^2 K_{uu}\Big)^{-1} K_{uf}}^{B} y \quad \text{(3)}
$$

We see here the *three layer network*: the first layer is the input, the second layer is formed by the kernel similarities to $X_u$ and the third one: the output $y$.

We may wonder why do we have to use the $K_{uu}$ in the regularization of the risk of $J(\alpha)$? One practical reason is that if $X_u$ is $X$ (i.e. all the data) the solution (3) will be the standard KRR solution since $K=K_{uf}=K_{fu}=K_{uu}$. Another one probably will be related to the fact that if two rows of $$X_u$$ $$ x_{ui} $$, $$ x_{uj} $$ are very correlated their corresponding $$\alpha_i$$, $$\alpha_j$$ coefficients should be regularized.

### Matrix inversion lemma proof

The easiest way to prove the equivalence between both methods (which is used in [Rassmusen and Williams book](http://www.gaussianprocess.org/gpml/){:target="_blank"} Chapter 8 section 6.1.) is to rely on the **matrix inversion lemma** which states:

$$
 \Big(Z + UWV^t\Big)^{-1} = Z^{-1} -Z^{-1} U\Big(W^{-1} + V^tZ^{-1}U\Big)^{-1}V^tZ^{-1}
$$

Hence we can apply this formula into the inverse of eq (2) $\Big( \overbrace{K_{fu}K_{uu}^{-1}K_{uf}}^{Q_{ff}}+\sigma^2 I\Big)^{-1}$ to get eq (3).


### Proof

The ad. hoc way to prove the result is showing that the matrices $A$ from (2) and $B$ from (3) are equivalent. We can simplify the expressions:

$$
A = K_{uu}^{-1}K_{uf}\Big(Q_{ff}+ \sigma^2 I \Big)^{-1} = K_{uu}^{-1}K_{uf}\Big(K_{fu}K_{uu}^{-1}K_{uf}+ \sigma^2 I \Big)^{-1}
$$

$$
\require{cancel}
B = \Big( K_{uf}K_{fu}\ +  \sigma^2 K_{uu}\Big)^{-1} K_{uf} = K_{uu}^{-1}\Big( K_{uf}K_{fu} K_{uu}^{-1} + \sigma^2 I \cancel{K_{uu}K_{uu}^{-1}} \Big)^{-1} K_{uf}
$$

Thus $A = B$ lead us to:

$$
\require{cancel}
\cancel{K_{uu}^{-1}} K_{uf}\Big(K_{fu}K_{uu}^{-1}K_{uf}+ \sigma^2 I \Big)^{-1} = \cancel{K_{uu}^{-1}} \Big( K_{uf}K_{fu} K_{uu}^{-1} + \sigma^2 I \Big)^{-1} K_{uf} \quad \text{(4)}
$$

Multiplying each part of the equation (4) at the left by $\Big( K_{uf}K_{fu} K_{uu}^{-1} + \sigma^2 I \Big)$ and at the right by $\Big(K_{fu}K_{uu}^{-1}K_{uf}+ \sigma^2 I \Big)$ we have:

$$
\Big( K_{uf}K_{fu} K_{uu}^{-1} + \sigma^2 I \Big) K_{uf} = K_{uf} \Big(K_{fu}K_{uu}^{-1}K_{uf}+ \sigma^2 I \Big)
$$

So the equality holds.

(This trick is taken from [http://stat.wikia.com/wiki/Kernel_Ridge_Regression](http://stat.wikia.com/wiki/Kernel_Ridge_Regression){:target="_blank"}).

## Bayesian approach

### Nystrom $\mathcal{GP}$
The Nystrom approach, in the context of $\mathcal{GP}$s, change the prior over $(y,y^{\*})$ with:

$$
p(y,y^* \mid X_*, X)=\mathcal{N}\left(\begin{pmatrix}y \\ y^* \end{pmatrix} \mid \; 0,\:\begin{pmatrix} Q_{f,f}+\sigma^2 I & Q_{f,*}\\ Q_{*,f} & Q_{*,*} + \sigma^2 I \end{pmatrix} \right)
$$

Here we want to retrieve the __posterior predictive distribution__: \\( p(y^* \mid X_*, X, y) \\) from this joint prior. If we dust off Christopher Bishop's: [Pattern Recognition and Machine Learning ](http://www.springer.com/us/book/9780387310732){:target="_blank"} book and we go to _chapter 2 section 2.3.1.: Conditional Gaussian distributions_ we have the derivation that gives the conditional distribution from a joint Gaussian distribution.

Applying this equation (which is also [here in the wikipedia](https://en.wikipedia.org/wiki/Multivariate_normal_distribution#Conditional_distributions)) leads to the expression of the **posterior predictive distribution**:

$$
p(y^* | X_*, X, y) = \mathcal{N}\big(f^* \mid Q_{*,f}(Q_{f,f}+\sigma^2 I)^{-1}y,\enspace Q_{*,*} - Q_{*,f}(Q_{f,f} + \sigma^2 I)^{-1}Q_{f,*} + \sigma^2 I \big) \quad \text{(5)}
$$

So we have that the mean of the $\mathcal{GP}$ solution using Nystrom kernel is the same as the aforementioned approaches.

### Bayesian RBFN
The RBFN method can be also viewed from a standard Bayesian linear regression point of view. The only _difference_ with the Bayesian linear regression is that we first transform the inputs $x_i$ to the similarities space $K_{x_iu}$. The rest remains equal. I am going to re-develop this approach for our current case because there are interesting similarities with the $\mathcal{GP}$ regression.

So let's start: we assume the _linear in $\alpha$_ model: $$y_i = K_{x_iu}\alpha + \epsilon_i$$ where $$\epsilon_i$$ is independent white noise: $\epsilon_i\sim \mathcal{N}(0,\sigma^2)$. This can also be written as $p(y_i\mid x_i,\alpha) = p(y_i \mid K_{x_iu},\alpha)=\mathcal{N}(y_i\mid K_{x_iu}\alpha, \sigma^2)$.
The likelihood of our model is then:

$$
\begin{align}
p(\overbrace{y_1,...,y_N}^y \mid \overbrace{x_1,...,x_N}^X,\alpha) &= \prod_{i=1}^N p(y_i\mid x_i,\alpha) \nonumber \\
&=  \prod_{i=1}^N \mathcal{N}(y_i \mid  K_{x_iu}\alpha , \sigma^2) = \mathcal{N}(y \mid K_{fu}\alpha, \sigma^2 I ) \nonumber \\
&= \frac{1}{(2\pi)^{N/2}\sqrt{|\sigma^2I|}}\exp\big(-\frac{1}{2}(y- K_{fu}\alpha)^t \sigma^{-2}I(y- K_{fu}\alpha)\big) \nonumber \\
&= \frac{1}{(2\pi \sigma^2)^{N/2}}\exp\big(-\frac{1}{2\sigma^2}\|y- K_{fu}\alpha\|^2\big) \nonumber \quad \text{(6)}
\end{align}
$$

If we place a prior over $p(\alpha \mid X) = \mathcal{N}(\alpha \mid 0,A)$, the joint $y,\alpha$ distribution given $X$ would be:

$$
p(y,\alpha \mid  X) = p(y\mid X,\alpha)p(\alpha \mid X)
$$

At this point it is worth to stop and consider *what do we want?* the natural answer is *give predictions for newcomming* \\( x_* \\) *values*. This means in Bayesian language that we want a **posterior predictive distribution** which is a probability distribution over the \\( y^* \\) values given all the available information:  \\( p(y^* \mid  X_\*, X, y)  \\).

One way to obtain the posterior predictive distribution, which is similar to the approach followed for $\mathcal{GP}$s is:

1. Augment the above equation with the test points:
$$ p(y,y^*,\alpha \mid  X,X_*) $$ (Changing the likelihood term to accommodate the unseen $$X^*$$ and $$y_*$$).
2. Integrate out \\( \alpha \\): $$ p(y,y^* \mid  X,X_*) = \int p(y,y^*,\alpha \mid  X,X_*) d\alpha $$
3. Compute the posterior predictive using the trick we used above in the Nystrom case (5).

The other, *more standard* approach would be:

1. Compute the posterior for $\alpha$: $p(\alpha \mid y,X) = p(y,\alpha\mid X)/p(y\mid X) $ (This can be done since all the formulas involve Gaussian distributions).
2. Compute the posterior predictive distribution integrating out $\alpha$:

$$
\begin{aligned}
p(y^* \mid x^* X, y) &= \int p(y^* \mid X^*, \alpha, \cancel{X, y}) P(\alpha \mid \cancel{X^*}, X, y) d \alpha \\&=
\int \mathcal{N}(y^*\mid K_{*u}\alpha, \sigma^2) p(\alpha \mid y,X)d \alpha
\end{aligned}
$$

We will further develop _similar to $\mathcal{GP}$ approach_ in the fully Bayesian approach section. However first we will consider the maximum a posteriori (MAP) approach which, we''ll see, it is equivalent to the empirical risk minimization approach.

#### MAP approach
The *"less Bayesian approach"* would be to compute the maximum a posteriori estimate of $p(\alpha \mid y,X)$ and then use such value ($\alpha_{MAP}$) to compute predictions: $y^* = K_{*,u}\alpha_{MAP}$. This is the approach which is equivalent to the empirical risk minimization approach exposed above (if we use $A=K_{uu}^{-1}$):

$$
\begin{aligned}
 \alpha_{MAP} &= \arg \max_{\alpha} p(\alpha \mid y,X) = \arg \max_{\alpha}\left[ \frac{p(y,\alpha \mid X)}{p(y\mid X)} \right] = \arg \max_{\alpha} p(y,\alpha \mid X) \\
 &= \arg \max_{\alpha} \big[\log(p(y,\alpha \mid X))\big] = \\
 &= \arg \min_{\alpha}\Big[ -\log(\overbrace{p(y \mid X, \alpha)}^{\text{likelihood}}) - \log(p(\alpha)) \Big] = \\
  &= \arg \min_{\alpha} \Big[  \frac{1}{\cancel{2}\sigma^2} \|y- K_{fu}\alpha\|^2 +  \cancel{\frac{N}{2}\log(2\pi \sigma)} +  \cancel{\frac{1}{2}}\alpha^t A^{-1} \alpha + \cancel{\frac{N}{2}\log(2\pi)}+  \cancel{\frac{1}{2}\log(|A|)}  \Big] \\
  &= \arg \min_{\alpha} J(\alpha)
 \end{aligned}
$$

So we see that the MAP of the Bayesian RBFN approach is the same of the empirical risk minimization RBFN approach. Given that we can also say that it is also the same to the KRR solution using Nystrom method and by the way it is also the mean of the $\mathcal{GP}$ solution using Nystrom method. So we have different methods derived from different points of view which at the end lead to the same *point estimates*. The only thing that remain to consider is the fully Bayesian RBFN approach; we wonder: *will the mean of fully Bayesian RBFN approach give the same predictions as all the other methods?* The answer, as you might expect, is also yes.

#### Fully Bayesian approach
We will develop here the *"similar to $\mathcal{GP}$"* approach to derive the **posterior predictive distribution** $$ p(y^*\mid x_*,X,y)$$. First we consider the *augmented likelihood*:

$$
\begin{equation}
p(y,y^* \mid  X, X_*,  \alpha) = \mathcal{N}\left(\begin{pmatrix} y \\ y^* \end{pmatrix} \Big | \; \begin{pmatrix} K_{fu} \\ K_{*u} \end{pmatrix} \alpha, \sigma^2 I \right)
\end{equation}
$$

Which is the same expression that (6) but including the test data $$(X_*,y^*)$$.
Now we integrate out $\alpha$:

$$
\begin{aligned}
  p(y,y^* \mid  X,X_*) &= \int p(y,y^* \alpha \mid  X,X_*) d\alpha = \int p(y,y^* \mid  X,X_*,\alpha)p(\alpha \mid X,X_*) d\alpha \\ &=
  \int \mathcal{N}\left(\begin{pmatrix} y \\ y^* \end{pmatrix} \Big | \; \begin{pmatrix} K_{fu} \\ K_{*u} \end{pmatrix} \alpha, \sigma^2 I \right) \mathcal{N}(\alpha \mid 0, A) d\alpha
\end{aligned}
$$

To solve this (nasty) integral we can rely again on Bishop's book in this case we will have to move to *section 2.3.3. Bayes' theorem for Gaussian variables*. However, if we *believe* that the convolution of two Gaussians is Gaussian we can just find the mean and covariance of $(y,y^* \mid X, X_*)$.

First we compute the mean:

$$
\mathbb{E}[\begin{pmatrix} y \\ y^* \end{pmatrix} \mid X, X_*] = \mathbb{E}[\mathbb{E}[\begin{pmatrix} y \\ y^* \end{pmatrix} \mid X, X_*,\alpha]] \underbrace{=}_{eq. 7} \mathbb{E}\left[\begin{pmatrix} K_{fu} \\ K_{*u} \end{pmatrix} \alpha + \epsilon \mid X, X_* \right] = \begin{pmatrix} K_{fu} \\ K_{*u} \end{pmatrix} \mathbb{E}[\alpha] + 0 \underbrace{=}_{\alpha \text{ has mean zero}} 0
$$

Then we compute the covariance:

$$
\begin{aligned}
\mathbb{E}\left[\begin{pmatrix} y \\ y^* \end{pmatrix} \begin{pmatrix}y^t & y^{*t} \end{pmatrix} \Big | \; X, X_*\right] &=
\mathbb{E}\left[\mathbb{E}\left[ \begin{pmatrix} y \\ y^* \end{pmatrix} \begin{pmatrix}y^t & y^{*t} \end{pmatrix} \Big | \; X, X_*,\alpha \right]\right] \\ &=
\mathbb{E}\left[ \begin{pmatrix} K_{fu}\alpha + \epsilon \\ K_{*u}\alpha + \epsilon \end{pmatrix} \begin{pmatrix} \alpha^tK_{uf} + \epsilon^t & \alpha^tK_{u*} + \epsilon^t \end{pmatrix} \Big | \; X, X_* \right] \\ &=
\mathbb{E}\left[ \begin{pmatrix} K_{fu} \\ K_{*u} \end{pmatrix} \alpha \alpha^T \begin{pmatrix}K_{uf}^t & K_{u*}^t \end{pmatrix} + \epsilon \epsilon^t \Big | \; X, X_*  \right]  \\  &=   
\begin{pmatrix} K_{fu} \\ K_{*u} \end{pmatrix} \mathbb{E}\left[\alpha \alpha^T\right] \begin{pmatrix}K_{uf}^t & K_{u*}^t \end{pmatrix} + \sigma^2 I  \\ &=
\begin{pmatrix} K_{fu} \\ K_{*u} \end{pmatrix} A \begin{pmatrix}K_{uf}^t & K_{u*}^t \end{pmatrix} + \sigma^2 I \\ &=
\begin{pmatrix} K_{fu}AK_{uf} & K_{fu}AK_{u*} \\ K_{*u}AK_{uf} & K_{*u}AK_{u*} \end{pmatrix} + \sigma^2 I
\end{aligned}
$$

Now we recognize here again that if $A=K_{uu}^{-1}$ we have the same joint distribution of the train test data:

$$
p(y,y^* \mid  X,X_*) = \mathcal{N}\left(\begin{pmatrix}y \\ y^* \end{pmatrix} \Big| \; 0,\:\begin{pmatrix} K_{fu}AK_{uf} + \sigma^2 I & K_{fu}AK_{u*} \\ K_{*u}AK_{uf} & K_{*u}AK_{u*} + \sigma^2 I \end{pmatrix} \right)
$$

Using the same procedure than in equation (5) and assuming $A=K_{uu}^{-1}$ we retrieve back the same **predictive posterior** as in Nystrom $\mathcal{GP}$ method.

One of the biggest concerns about this predictive posterior distribution (equation 5) is that the predictive variance may go to zero when the test input $$x^*$$ is far from the the subset $X_u$. To see why consider that if $$x^*$$ is far from all the data $X$ the natural behavior of the $\mathcal{GP}$ is to stick to the prior. The prior variance is $$Q_{*,*} = K_{*u}K_{uu}^{-1}K_{u*}$$ which in turn will be close to zero if we consider for example the *rbf* kernel. We refer the reader to [[Quiñonero-Candela 2005]](http://www.jmlr.org/papers/v6/quinonero-candela05a.html){:target="_blank"} for further discussion and alternatives.
